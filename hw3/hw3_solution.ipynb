{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Xiyao Xu\n",
    "\n",
    "06/07/2024"
   ],
   "id": "5456016497154988"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Homework 3 - Scraping and Regression\n",
    "\n",
    "## Part 1 - Data Acquisition"
   ],
   "id": "a18c8f5bfe34013a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import datetime\n",
    "from math import floor\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def calculate_time_gap(timestamp_string):\n",
    "    timestamp = datetime.datetime.strptime(timestamp_string, \"%Y-%m-%dT%H:%M:%S\")\n",
    "    current_time = datetime.datetime.now()\n",
    "    time_gap = (current_time - timestamp).total_seconds() / 3600\n",
    "    return floor(time_gap)\n",
    "\n",
    "\n",
    "hackernews_file = 'hackernews_stories.csv'\n",
    "\n",
    "\n",
    "def scrape_hackernews_stories():\n",
    "    stories_data = []\n",
    "    for page in range(1, 6):\n",
    "        url = f\"http://news.ycombinator.com/news?p={page}\"\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        story_rows = soup.select(\".athing\")\n",
    "\n",
    "        for row in story_rows:\n",
    "            rank = row.find(class_=\"rank\").text.strip(\".\")\n",
    "            title = row.find(class_=\"titleline\").text\n",
    "            next_row = row.find_next_sibling(\"tr\")\n",
    "\n",
    "            if next_row:\n",
    "                original_age_string = next_row.find(class_=\"age\")['title']\n",
    "                age = calculate_time_gap(original_age_string)\n",
    "                points = int(next_row.find(class_=\"score\").text.split()[0]) if next_row.find(class_=\"score\") else 0\n",
    "\n",
    "                comments_link = next_row.find(\"a\", href=lambda href: href and \"item?id\" in href)\n",
    "                if comments_link:\n",
    "                    comments_text = comments_link.text.strip()\n",
    "                    comments = int(comments_text.split()[0]) if comments_text.split()[0].isdigit() else 0\n",
    "                else:\n",
    "                    comments = 0\n",
    "            else:\n",
    "                age = 0\n",
    "                points = 0\n",
    "                comments = 0\n",
    "\n",
    "            stories_data.append({\n",
    "                \"Rank\": rank,\n",
    "                \"Title\": title,\n",
    "                \"Age(hours)\": age,\n",
    "                \"Points\": points,\n",
    "                \"Comments\": comments\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(stories_data)\n",
    "    df.to_csv(hackernews_file, index=False)\n",
    "    print(f\"Scraping completed. Data saved to {hackernews_file}.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_hackernews_stories()"
   ],
   "id": "1aa9c6780107d22b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Part 2 - Regression",
   "id": "26ab5bc454744d25"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import itertools\n",
    "\n",
    "# 读取数据\n",
    "df = pd.read_csv('hackernews_stories.csv')\n",
    "\n",
    "# 数据探索\n",
    "print(df.describe())\n",
    "\n",
    "# 选择只包含数值列的子集\n",
    "numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "print(df[numeric_columns].corr())\n",
    "\n",
    "# 计算标题长度\n",
    "df['Title_Length'] = df['Title'].apply(len)\n",
    "\n",
    "# 准备特征和目标变量\n",
    "X = df[['Points', 'Comments', 'Title_Length']]\n",
    "y = df['Rank']\n",
    "\n",
    "# 线性回归\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)\n",
    "y_pred_lin = lin_reg.predict(X)\n",
    "print(f\"Linear Regression Coefficients: {lin_reg.coef_}\")\n",
    "print(f\"Linear Regression R-squared: {r2_score(y, y_pred_lin)}\")\n",
    "\n",
    "# 多项式回归\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_poly = poly.fit_transform(X)\n",
    "poly_reg = LinearRegression()\n",
    "poly_reg.fit(X_poly, y)\n",
    "y_pred_poly = poly_reg.predict(X_poly)\n",
    "print(f\"Polynomial Regression R-squared: {r2_score(y, y_pred_poly)}\")\n",
    "\n",
    "# 岭回归\n",
    "ridge = Ridge(alpha=1.0)\n",
    "ridge.fit(X, y)\n",
    "y_pred_ridge = ridge.predict(X)\n",
    "print(f\"Ridge Regression Coefficients: {ridge.coef_}\")\n",
    "print(f\"Ridge Regression R-squared: {r2_score(y, y_pred_ridge)}\")\n",
    "\n",
    "# 绘制实际rank与预测rank的散点图\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.scatter(y, y_pred_lin)\n",
    "plt.xlabel('Actual Rank')\n",
    "plt.ylabel('Predicted Rank')\n",
    "plt.title('Linear Regression')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.scatter(y, y_pred_poly)\n",
    "plt.xlabel('Actual Rank')\n",
    "plt.ylabel('Predicted Rank')\n",
    "plt.title('Polynomial Regression')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.scatter(y, y_pred_ridge)\n",
    "plt.xlabel('Actual Rank')\n",
    "plt.ylabel('Predicted Rank')\n",
    "plt.title('Ridge Regression')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 尝试不同的特征组合\n",
    "features = ['Points', 'Comments', 'Age(hours)', 'Title_Length']\n",
    "for i in range(1, len(features)+1):\n",
    "    for combo in itertools.combinations(features, i):\n",
    "        X = df[list(combo)]\n",
    "        lin_reg = LinearRegression()\n",
    "        lin_reg.fit(X, y)\n",
    "        y_pred = lin_reg.predict(X)\n",
    "        print(f\"Features: {combo}, R-squared: {r2_score(y, y_pred)}\")"
   ],
   "id": "42b9ff4dd93a6bd0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Comparison of Regression Models:\n",
    "1. Linear Regression: The linear regression model assumes a linear relationship between the features and the target variable. It achieved an R-squared score of 0.1479, indicating that around 14.79% of the variance in the rank can be explained by the linear combination of the features (Points, Comments, Title_Length).\n",
    "\n",
    "2. Polynomial Regression: The polynomial regression model with degree 2 allows for capturing non-linear relationships between the features and the target variable. It achieved a higher R-squared score of 0.2825, suggesting that the polynomial features can better explain the variance in the rank compared to the linear regression model.\n",
    "\n",
    "3. Ridge Regression: The ridge regression model is similar to linear regression but with L2 regularization to handle multicollinearity and prevent overfitting. It achieved an R-squared score of 0.1479, which is the same as the linear regression model. This suggests that the regularization did not significantly improve the model's performance.\n",
    "\n",
    "Among the three models, the polynomial regression model appears to be the most useful as it captures non-linear relationships and achieves the highest R-squared score.\n",
    "\n",
    "Relationships between Variables:\n",
    "1. Linear Relationships:\n",
    "   - The correlation matrix shows weak to moderate correlations between the variables.\n",
    "   - Points and Comments have a weak negative correlation with Rank (-0.1913 and -0.3163, respectively), suggesting that higher points and more comments are slightly associated with lower ranks (higher positions).\n",
    "   - Age(hours) has a weak positive correlation with Rank (0.1670), indicating that older stories tend to have slightly higher ranks.\n",
    "   - The linear regression coefficients also suggest weak relationships between the features and the rank.\n",
    "\n",
    "2. Inverse Linear Relationships (1/x):\n",
    "   - The analysis does not explicitly explore inverse linear relationships.\n",
    "   - However, I could create new features by taking the reciprocal of the existing features (e.g., 1/Points, 1/Comments) and include them in the regression models to see if they improve the model's performance.\n",
    "\n",
    "Feature Combinations:\n",
    "- The analysis explored different feature combinations and reported their R-squared scores.\n",
    "- The combination of Points, Comments, and Age(hours) achieved the highest R-squared score of 0.1665 among the combinations with three features.\n",
    "- The combination of all four features (Points, Comments, Age(hours), Title_Length) achieved a slightly higher R-squared score of 0.1680.\n",
    "- These results suggest that the combination of Points, Comments, and Age(hours) provides the most useful information for predicting the rank, while Title_Length contributes marginally.\n",
    "\n",
    "In summary, the polynomial regression model appears to be the most useful among the three models, capturing non-linear relationships and achieving the highest R-squared score. The analysis reveals weak to moderate linear relationships between the variables and the rank. The combination of Points, Comments, and Age(hours) seems to be the most informative for predicting the rank. Further exploration of inverse linear relationships and other non-linear relationships could potentially improve the model's performance."
   ],
   "id": "aaf99410ebf445f4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Part 3 - Classification",
   "id": "4abcb74e802c36a7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# 读取数据\n",
    "df = pd.read_csv('hackernews_stories.csv')\n",
    "\n",
    "# 创建一个新的二元特征 'is_front_page'，表示文章是否在首页（排名 <= 30）\n",
    "df['is_front_page'] = (df['Rank'] <= 30).astype(int)\n",
    "\n",
    "# 计算标题长度\n",
    "df['Title_Length'] = df['Title'].apply(len)\n",
    "\n",
    "# 准备特征和目标变量\n",
    "X = df[['Points', 'Comments', 'Title_Length']]\n",
    "y = df['is_front_page']\n",
    "\n",
    "# 划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 训练逻辑回归模型\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# 在测试集上进行预测\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "# 评估模型性能\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-score: {f1}\")\n",
    "\n",
    "# 绘制混淆矩阵\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(2)\n",
    "plt.xticks(tick_marks, ['Not Front Page', 'Front Page'])\n",
    "plt.yticks(tick_marks, ['Not Front Page', 'Front Page'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n",
    "\n",
    "# 绘制不同特征的logistic曲线\n",
    "features = ['Points', 'Comments', 'Title_Length']\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "for i, feature in enumerate(features):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    X_single = df[[feature]].values\n",
    "    log_reg_single = LogisticRegression(random_state=42)\n",
    "    log_reg_single.fit(X_single, y)\n",
    "\n",
    "    x_min, x_max = X_single.min(), X_single.max()\n",
    "    xx = np.linspace(x_min, x_max, 100)\n",
    "    y_proba = log_reg_single.predict_proba(xx.reshape(-1, 1))[:, 1]\n",
    "\n",
    "    plt.plot(xx, y_proba, color='blue', linewidth=2)\n",
    "    plt.scatter(X_single, y, color='red', alpha=0.3)\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Probability of Being on Front Page')\n",
    "    plt.title(f'Logistic Curve for {feature}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "f0aa3ed399195474"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Based on the code and output for Part 3, include plots showing the logistic regression curves for individual features (Points, Comments, and Title_Length) in relation to the probability of an article making it to the front page.\n",
    "\n",
    "1. Logistic Curve for Points:\n",
    "   - The curve shows an increasing trend, indicating that articles with higher points have a higher probability of appearing on the front page.\n",
    "   - As the number of points increases, the likelihood of an article making it to the front page also increases.\n",
    "\n",
    "2. Logistic Curve for Comments:\n",
    "   - Similar to the Points curve, the Comments curve also exhibits an increasing trend.\n",
    "   - Articles with a higher number of comments have a higher probability of being on the front page.\n",
    "   - The relationship between the number of comments and the probability of making it to the front page appears to be positive.\n",
    "\n",
    "3. Logistic Curve for Title_Length:\n",
    "   - The curve for Title_Length is relatively flat compared to the curves for Points and Comments.\n",
    "   - This suggests that the length of the title has a lesser impact on the probability of an article appearing on the front page.\n",
    "   - The relationship between title length and front page probability seems to be weaker compared to the other two features.\n",
    "\n",
    "What the regressions tell about making the front page:\n",
    "- The logistic regression curves provide insights into the factors that influence an article's chances of making it to the front page of Hacker News.\n",
    "- The curves for Points and Comments indicate that articles with higher scores and more comments have a higher probability of appearing on the front page. This suggests that user engagement (in the form of upvotes and comments) plays a significant role in determining an article's visibility and popularity.\n",
    "- On the other hand, the Title_Length curve suggests that the length of the title has a relatively minor impact on an article's chances of making it to the front page. While a catchy or informative title may be important, the length of the title itself does not seem to be a strong predictor of front page success.\n",
    "- Overall, the regressions highlight the importance of user engagement metrics (Points and Comments) in driving an article's visibility and popularity on Hacker News. Articles that generate more upvotes and discussions tend to have a higher likelihood of reaching the front page.\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "f12c1f9cc99efd18"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Part 4 - Fun extra challenge",
   "id": "b5ec701d95e19783"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# 加载数据\n",
    "df = pd.read_csv('hackernews_stories.csv')\n",
    "\n",
    "# 实现排名公式\n",
    "def rank_formula(points, age, comments):\n",
    "    gravity = 1.8\n",
    "    time_decay = 0.95\n",
    "    return (points - 1) / (age + 2) ** gravity + comments / (age + 2) ** time_decay\n",
    "\n",
    "# 计算 \"真实\" 排名\n",
    "df['true_rank'] = rank_formula(df['Points'], df['Age(hours)'], df['Comments'])\n",
    "\n",
    "# 准备特征和目标变量\n",
    "X = df[['Points', 'Age(hours)', 'Comments']]\n",
    "y = df['true_rank']\n",
    "\n",
    "# 执行最小二乘回归\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# 比较估计的系数与实际系数\n",
    "estimated_coefficients = model.coef_\n",
    "actual_coefficients = [1, -1.8, 1]  # 根据排名公式的实际系数\n",
    "print(\"Estimated Coefficients:\", estimated_coefficients)\n",
    "print(\"Actual Coefficients:\", actual_coefficients)\n",
    "\n",
    "# 计算误差度量或差异\n",
    "coefficient_errors = estimated_coefficients - actual_coefficients\n",
    "print(\"Coefficient Errors:\", coefficient_errors)\n",
    "\n",
    "# 计算均方误差 (MSE)\n",
    "y_pred = model.predict(X)\n",
    "mse = np.mean((y_pred - y) ** 2)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# 解释结果\n",
    "print(\"\\nResults Interpretation:\")\n",
    "print(\"- The estimated coefficients from the linear regression are:\")\n",
    "print(\"  Points: {:.2f}, Age(hours): {:.2f}, Comments: {:.2f}\".format(*estimated_coefficients))\n",
    "print(\"- The actual coefficients used in the ranking formula are:\")\n",
    "print(\"  Points: {}, Age(hours): {}, Comments: {}\".format(*actual_coefficients))\n",
    "print(\"- The coefficient errors (estimated - actual) are:\")\n",
    "print(\"  Points: {:.2f}, Age(hours): {:.2f}, Comments: {:.2f}\".format(*coefficient_errors))\n",
    "print(\"- The Mean Squared Error (MSE) between the predicted and true ranks is: {:.2f}\".format(mse))\n",
    "print(\"- The linear regression model tries to estimate the coefficients of the ranking formula.\")\n",
    "print(\"- The closer the estimated coefficients are to the actual coefficients, the better the model.\")\n",
    "print(\"- A lower MSE indicates that the predicted ranks are closer to the true ranks.\")\n",
    "print(\"- The results suggest that the linear regression model can approximate the ranking formula,\")\n",
    "print(\"  but there are still some differences between the estimated and actual coefficients.\")\n",
    "print(\"- Factors such as data quality, sample size, and the complexity of the ranking formula\")\n",
    "print(\"  can affect the accuracy of the coefficient estimation.\")"
   ],
   "id": "a8c6a8501dd04338"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
